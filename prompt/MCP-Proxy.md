### 总体架构设计

本方案设计一个基于MCP（Model Context Protocol）的API网关系统，采用模块化架构，确保高可扩展性、可维护性和安全性。系统整体分为四个核心模块：服务代理层、服务管理门户、鉴权体系和流量治理模块。这些模块通过微服务架构松耦合，采用事件驱动和API接口进行交互。底层基础设施包括：

- **技术栈**：后端使用Java（Spring Boot）实现高性能代理；前端使用React.js构建门户；数据库采用MySQL（存储配置、日志）和Redis（缓存限流计数）；监控使用Prometheus + Grafana。
- **部署模式**：Kubernetes容器化，支持水平扩展。
- **通信协议**：外部暴露HTTP/HTTPS，内部使用MCP协议转换。
- **安全性**：全链路TLS加密，集成OAuth2.0扩展鉴权。
- **高可用性**：多实例部署，结合服务发现（如Consul）实现负载均衡。

系统架构图描述（文本表示）：

```
[外部客户端 (HTTP/HTTPS)] --> [MCP代理网关 (服务代理层)] --> [后端MCP服务]
                                      |
                                      v
[服务管理门户] <--> [鉴权体系] <--> [流量治理模块]
                                      |
                                      v
[数据库/缓存/监控]
```

每个模块独立开发，可独立部署和升级。

### 一、服务代理层

该模块负责协议转换和代理转发，是网关的核心入口点。设计为 stateless 服务，便于扩展。

#### 1.1 模块组件
- **协议转换器**：处理HTTP/HTTPS请求到MCP协议的转换。
  - 输入：HTTP请求（方法、路径、头、Body）。
  - 输出：MCP消息（序列化后转发到后端服务）。
  - 实现：使用自定义MCP库（假设基于Protobuf或Thrift序列化）。转换逻辑包括：
    - 映射HTTP方法到MCP操作码（e.g., GET -> Query）。
    - 头信息保留（如User-Agent），并添加MCP特定元数据（e.g., Trace ID）。
    - Body转换：JSON/XML 到 MCP二进制。

#### 1.2 工作流程
1. 接收HTTP/HTTPS请求。
2. 调用鉴权模块验证（详见三）。
3. 调用流量治理模块检查限流（详见四）。
4. 执行协议转换并转发到MCP服务。
5. 接收MCP响应，转换回HTTP格式返回客户端。
6. 记录调用日志（集成到鉴权记录中）。

#### 1.3 性能优化
- 异步处理：使用JAVA Reactor模式。
- 缓存：Redis缓存热门路由映射。
- 错误处理：统一返回HTTP状态码（e.g., 429 for rate limit）。

### 二、服务管理门户

该模块提供用户界面，用于服务发现和管理。设计为前后端分离的Web应用。

#### 2.1 模块组件
- **服务市场UI**：React-based dashboard。
  - 服务详情展示：实时拉取监控数据。
    - QPS：从Prometheus查询当前/历史QPS。
    - 延迟：P50/P95/P99延迟指标。
- **配置生成器**：支持多选服务生成MCP Client配置。
  - 输入：用户选中服务列表。
  - 输出：YAML/JSON文件下载。
    - 示例YAML格式：
      ```
      services:
        - id: service1
          endpoint: mcp://host:port
          auth: { key: "generated_key" }
        - id: service2
          ...
      ```
    - 逻辑：查询数据库服务元数据，结合用户ID生成密钥（详见三），序列化为YAML/JSON。

#### 2.2 工作流程
1. 浏览服务市场：API调用后端查询服务列表。
2. 查看详情：异步加载指标和文档。
3. 生成配置：选中服务 -> 调用后端API生成文件 -> 下载。
4. 权限控制：仅授权用户可见敏感数据。

### 三、鉴权体系

该模块处理身份验证和调用记录，集成到代理层中作为中间件。

#### 3.1 模块组件
- **密钥生成器**：
  - 格式：基于MCP服务ID + 用户ID生成（e.g., HMAC-SHA256(service_id + user_id + salt)）。
  - 生命周期：默认永久有效，支持手动失效（存储在数据库，添加expire字段）。
  - 生成逻辑：门户调用时自动生成，存储到MySQL（表：auth_keys，字段：key, user_id, service_id, created_at, expires_at）。
- **请求验证器**：
  - 支持两种方式：
    - Query参数：e.g., ?auth_key=xxx。
    - Authorization Header：e.g., Authorization: Bearer xxx。
  - 验证流程：
    1. 提取key。
    2. 查询数据库验证key有效性（匹配user_id, service_id，未过期）。
    3. 若无效，返回401 Unauthorized。
- **调用记录器**：
  - 每次调用记录<user_id, service_id, timestamp, status_code>。
  - 存储：MySQL表（call_logs），支持分区存储以优化查询。
  - 异步写入：使用Kafka队列缓冲，防止阻塞主流程。
  - 保留期：默认30天，支持配置。

#### 3.2 工作流程
1. 代理层接收请求 -> 提取鉴权信息。
2. 调用验证器检查。
3. 若通过，记录日志（成功/失败）。
4. 集成到流量治理（user_id用于限流）。

#### 3.3 安全强化
- 密钥加密存储：使用AES加密。
- 防重放：添加nonce或timestamp检查。
- 审计：定期导出日志用于分析。

### 四、流量治理

该模块实现限流策略，基于Redis分布式计数器，确保全局一致性。

#### 4.1 模块组件
- **限流规则引擎**：使用Lua脚本在Redis执行原子操作。
  - 全局维度：单服务最大QPS（e.g., Redis key: "global:service_id:qps"，使用滑动窗口算法）。
  - 用户维度：单用户最大QPS（key: "user:user_id:qps"）。
  - 混合规则：用户+服务组合（key: "user_service:user_id:service_id:qps"）。
- **配置管理**：从数据库加载规则，支持热更新（e.g., 全局QPS=1000，用户QPS=100，组合=50）。
- **熔断/降级**：扩展支持Hystrix-like熔断，当QPS超标时返回降级响应。

#### 4.2 工作流程
1. 代理层接收请求 -> 提取user_id, service_id。
2. 并行检查三维度限流：
   - 若任一超出，返回429 Too Many Requests。
3. 通过后，递增计数器。
4. 请求完成后，更新监控指标。

#### 4.3 限流算法比较（使用表格呈现）

| 维度       | 算法类型     | 优点                     | 缺点                     | 配置示例 |
|------------|--------------|--------------------------|--------------------------|----------|
| 全局      | 滑动窗口    | 平滑处理突发流量        | 内存消耗稍高            | max_qps=1000, window=1s |
| 用户      | 令牌桶      | 支持突发，易配置        | 需要定时填充令牌        | max_qps=100, burst=200 |
| 混合      | 漏桶        | 严格控制速率            | 不支持突发              | max_qps=50, leak_rate=1/s |

#### 4.4 监控与告警
- 集成Prometheus exporter，暴露限流指标。
- 告警：当接近阈值80%时，推送通知。

### 实施与测试计划
- **开发阶段**：模块化开发，使用Docker Compose本地测试。
- **测试**：单元测试（覆盖率>80%）、集成测试（模拟高并发）、压力测试（JMeter模拟QPS）。
- **上线**：灰度发布，监控关键指标（延迟<50ms，成功率>99.9%）。
- **风险**：协议转换兼容性问题 -> 通过版本化MCP解决；限流误判 -> 添加白名单。

此方案确保满足所有需求，支持未来扩展如A/B测试或AI-based流量优化。