### 总体架构设计

本方案设计一个基于MCP（Model Context Protocol）的API网关系统，采用模块化架构，确保高可扩展性、可维护性和安全性。系统整体分为三个核心模块：服务代理层、服务管理模块、鉴权模块。这些模块通过微服务架构松耦合，采用事件驱动和API接口进行交互。底层基础设施包括：

- **技术栈**：后端使用Java17 +Spring Boot 3.5.4 + mybatis + maven 实现高性能代理；数据库采用MySQL（存储配置、日志）和Redis（缓存限流计数）。
- **部署模式**： 打包成fatjar部署。
- **通信协议**：外部暴露HTTP/HTTPS，内部使用MCP协议转换。
- **安全性**：全链路TLS加密，集成OAuth2.0扩展鉴权。

每个模块独立开发，可独立部署和升级。

### 一、服务代理层

该模块负责协议转换和代理转发，是网关的核心入口点。设计为 stateless 服务，便于扩展。

#### 1.1 模块组件
- **协议转换器**：处理HTTP/HTTPS请求到MCP协议的转换。
    - 输入：HTTP请求（方法、路径、头、Body）。
    - 输出：MCP消息（序列化后转发到后端服务）。
    - 实现：使用自定义MCP库（假设基于Protobuf或Thrift序列化）。转换逻辑包括：
        - 映射HTTP方法到MCP操作码（e.g., GET -> Query）。
        - 头信息保留（如User-Agent），并添加MCP特定元数据（e.g., Trace ID）。
        - Body转换：JSON/XML 到 MCP二进制。

#### 1.2 工作流程
1. 接收HTTP/HTTPS请求。
2. 调用鉴权模块验证（详见三）。
3. 执行协议转换并转发到MCP服务。
4. 接收MCP响应，转换回HTTP格式返回客户端。

#### 1.3 性能优化
- 异步处理：使用JAVA Reactor模式。
- 缓存：Redis缓存热门路由映射。
- 错误处理：统一返回HTTP状态码（e.g., 429 for rate limit）。

### 二、服务管理模块

该模块提供后端接口，用于服务发现和管理。设计为前后端分离的Web应用，前端无需考虑

#### 2.1 模块组件
- **服务市场接口**：
    - 服务详情展示：实时拉取监控数据。
- **配置生成器**：支持多选服务生成MCP Client配置。
    - 输入：用户选中服务列表。
    - 输出：YAML/JSON文件下载。
        - 示例YAML格式：
          ```
          services:
            - id: service1
              endpoint: mcp://host:port
              auth: { key: "generated_key" }
            - id: service2
              ...
          ```
        - 逻辑：查询数据库服务元数据，结合用户ID生成密钥（详见三），序列化为YAML/JSON。

#### 2.2 工作流程
1. 前端查询服务市场：后端查询服务列表。
2. 查看详情：异步加载指标和文档。
3. 生成配置：选中服务 -> 调用后端API生成文件 -> 下载。
4. 权限控制：仅授权用户可见敏感数据。

### 三、鉴权模块

该模块处理身份验证和调用记录，集成到代理层中作为中间件。

#### 3.1 模块组件
- **密钥生成器**：
    - 格式：基于MCP服务ID + 用户ID生成（e.g., HMAC-SHA256(service_id + user_id + salt)）。
    - 生命周期：默认永久有效，支持手动失效（存储在数据库，添加expire字段）。
    - 生成逻辑：门户调用时自动生成，存储到MySQL（表：auth_keys，字段：key, user_id, service_id, created_at, expires_at）。
- **请求验证器**：
    - 支持两种方式：
        - Query参数：e.g., ?auth_key=xxx。
        - Authorization Header：e.g., Authorization: Bearer xxx。
    - 验证流程：
        1. 提取key。
        2. 查询数据库验证key有效性（匹配user_id, service_id，未过期）。
        3. 若无效，返回401 Unauthorized。
- **调用记录器**：
    - 每次调用记录<user_id, service_id, timestamp, status_code>。
    - 异步写入：写入本地的异步阻塞队列即可，消费无需考虑。

#### 3.2 工作流程
1. 代理层接收请求 -> 提取鉴权信息。
2. 调用验证器检查。
3. 若通过，记录日志（成功/失败）。
4. 集成到流量治理（user_id用于限流）。

#### 3.3 安全强化
- 密钥加密存储：使用AES加密。
- 防重放：添加nonce或timestamp检查。
### 四、流量治理

该模块实现限流策略，基于Redis分布式计数器，确保全局一致性。

#### 4.1 模块组件
- **限流规则引擎**：使用Lua脚本在Redis执行原子操作。
    - 全局维度：单服务最大QPS（e.g., Redis key: "global:service_id:qps"，使用滑动窗口算法）。
    - 用户维度：单用户最大QPS（key: "user:user_id:qps"）。
    - 混合规则：用户+服务组合（key: "user_service:user_id:service_id:qps"）。
- **配置管理**：从数据库加载规则，支持热更新（e.g., 全局QPS=1000，用户QPS=100，组合=50）。
- **熔断/降级**：扩展支持sentinel熔断，当QPS超标时返回降级响应。

#### 4.2 工作流程
1. 代理层接收请求 -> 提取user_id, service_id。
2. 并行检查三维度限流：
    - 若任一超出，返回429 Too Many Requests。
3. 通过后，递增计数器。
4. 请求完成后，更新监控指标。

#### 4.3 限流算法比较（使用表格呈现）

| 维度       | 算法类型     | 优点                     | 缺点                     | 配置示例 |
|------------|--------------|--------------------------|--------------------------|----------|
| 全局      | 滑动窗口    | 平滑处理突发流量        | 内存消耗稍高            | max_qps=1000, window=1s |
| 用户      | 令牌桶      | 支持突发，易配置        | 需要定时填充令牌        | max_qps=100, burst=200 |
| 混合      | 漏桶        | 严格控制速率            | 不支持突发              | max_qps=50, leak_rate=1/s |